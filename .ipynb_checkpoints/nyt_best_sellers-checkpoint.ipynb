{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fitted-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-object",
   "metadata": {},
   "source": [
    "Psuedo Code Plan:\n",
    "On every page, scrape:\n",
    "- book title\n",
    "- author\n",
    "- how many weeks on list\n",
    "- The date of the list (easy)\n",
    "- The book's position on the list\n",
    "\n",
    "\n",
    "Good plan! Seems very do-able. One thing to think about is the \"number of weeks on the list\" data and the fact that you will have repeated books in your data (which is fine, but think about how to deal with that). I want you definitely to store two additional items in each record: 1) The date of the list (easy) 2) The book's position on the list (look at the HTML structure and think how you would derive this number to store) That would give you more to analyze, if an analysis were to be done.\n",
    "\n",
    "On every page, scrape the information\n",
    "\n",
    "get_list_links\n",
    "- gather list of links for past x weeks\n",
    "- iterate to find links\n",
    "- will produce list of links\n",
    "\n",
    "get_books\n",
    "- iterate through links to get list of books\n",
    "\n",
    "get_book_details\n",
    "\n",
    "\n",
    "\n",
    "CSV\n",
    "\n",
    "Title, Author, Date of List, Book Position, Weeks on List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "awful-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome('/Users/tienle/Documents/python/jupyter_work/chromedriver')\n",
    "# driver.get('http://www.google.com/');\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "designing-recovery",
   "metadata": {},
   "source": [
    "test_url = '/books/best-sellers/2021/02/28/combined-print-and-e-book-fiction/'\n",
    "\n",
    "def get_book_details(week_url):\n",
    "    page = requests.get('https://www.nytimes.com/' + week_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    ## book titles list\n",
    "    get_titles = soup.find_all(\"h3\", class_=\"css-5pe77f\")\n",
    "    for title in get_titles:\n",
    "        print(title.get_text())\n",
    "        #assign variable > put variable in list > but iterate through larger list of books\n",
    "        # for every book, put its details > loop\n",
    "    \n",
    "    ## get author\n",
    "    get_authors = soup.find_all(\"p\", class_=\"css-hjukut\")\n",
    "    for author in get_authors:\n",
    "        author = author.get_text()\n",
    "        print(author[3:]) #slice 'by' out of author name\n",
    "        \n",
    "    ## date of the list \n",
    "    get_date = soup.find(\"time\", class_=\"css-6068ga\")\n",
    "    print(get_date.get_text())\n",
    "\n",
    "    \n",
    "    ## how many weeks book has been on list\n",
    "    get_position = soup.find_all(\"p\", class_=\"css-1o26r9v\")\n",
    "    for position in get_position:\n",
    "        print(position.get_text())\n",
    "    # can do if-else of whether book is new or contains number\n",
    "    \n",
    "    \n",
    "    ## book's position on the list\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "book_details = get_book_details(test_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "controlled-florida",
   "metadata": {},
   "source": [
    "test_url = '/books/best-sellers/2021/02/28/combined-print-and-e-book-fiction/'\n",
    "\n",
    "def get_book_details(week_url):\n",
    "    page = requests.get('https://www.nytimes.com/' + week_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    get_books = soup.find_all(\"li\", class_=\"css-13y32ub\") #get entire list of books\n",
    "    \n",
    "    for book in get_books:\n",
    "        \n",
    "        get_titles = soup.find(\"h3\", class_=\"css-5pe77f\") ## get title and rank\n",
    "        rank = 0 \n",
    "#         for title in get_titles: #for every book, get the title and rank\n",
    "        title = get_titles.get_text()\n",
    "#             print(title.get_text())\n",
    "        rank += 1\n",
    "\n",
    "        get_authors = soup.find(\"p\", class_=\"css-hjukut\")\n",
    "#         for author in get_authors:\n",
    "        author = get_authors.get_text()\n",
    "        author = author[3:] #slice 'by' out of author name\n",
    "\n",
    "            ## date of the list \n",
    "        get_date = soup.find(\"time\", class_=\"css-6068ga\")\n",
    "        date = get_date.get_text()    \n",
    "\n",
    "            ## how many weeks book has been on list\n",
    "        get_weeks_on = soup.find(\"p\", class_=\"css-1o26r9v\")\n",
    "#         for week in get_weeks_on:\n",
    "        weeks_on = get_weeks_on.get_text()\n",
    "            \n",
    "        details = [title, author, rank, date, weeks_on]\n",
    "        return details\n",
    "\n",
    "\n",
    "book_details = get_book_details(test_url)\n",
    "print(book_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "beneficial-empire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE FOUR WINDS', 'Kristin Hannah', '#1', 'February 28, 2021', '2']\n",
      "['FAITHLESS IN DEATH', 'J.D. Robb', '#2', 'February 28, 2021', 'New']\n",
      "['FIREFLY LANE', 'Kristin Hannah', '#3', 'February 28, 2021', '4']\n",
      "['RECKLESS ROAD', 'Christine Feehan', '#4', 'February 28, 2021', 'New']\n",
      "['THE DUKE AND I', 'Julia Quinn', '#5', 'February 28, 2021', '7']\n",
      "['THE MIDNIGHT LIBRARY', 'Matt Haig', '#6', 'February 28, 2021', '11']\n",
      "['RAFAEL', 'Laurell K. Hamilton', '#7', 'February 28, 2021', 'New']\n",
      "['THE SANATORIUM', 'Sarah Pearse', '#8', 'February 28, 2021', '2']\n",
      "['THE VISCOUNT WHO LOVED ME', 'Julia Quinn', '#9', 'February 28, 2021', '7']\n",
      "['THE VANISHING HALF', 'Brit Bennett', '#10', 'February 28, 2021', '37']\n",
      "['THE RUSSIAN', 'James Patterson and James O. Born', '#11', 'February 28, 2021', '3']\n",
      "['THE PARIS LIBRARY', 'Janet Skeslien Charles', '#12', 'February 28, 2021', 'New']\n",
      "['THE INVISIBLE LIFE OF ADDIE LARUE', 'V.E. Schwab', '#13', 'February 28, 2021', '14']\n",
      "['ROMANCING MISTER BRIDGERTON', 'Julia Quinn', '#14', 'February 28, 2021', '5']\n",
      "['WHERE THE CRAWDADS SING', 'Delia Owens', '#15', 'February 28, 2021', '121']\n"
     ]
    }
   ],
   "source": [
    "test_url = '/books/best-sellers/2021/02/28/combined-print-and-e-book-fiction/'\n",
    "\n",
    "def get_book_details(week_url):\n",
    "    page = requests.get('https://www.nytimes.com/' + week_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    get_ol = soup.find(\"ol\", class_=\"css-12yzwg4\")\n",
    "    \n",
    "    get_books = get_ol.find_all(\"li\", class_=\"css-13y32ub\")\n",
    "    get_titles = soup.find_all(\"h3\", class_=\"css-5pe77f\")\n",
    "    get_authors = soup.find_all(\"p\", class_=\"css-hjukut\")\n",
    "    get_date = soup.find(\"time\", class_=\"css-6068ga\")\n",
    "    get_positions = soup.find_all(\"p\", class_=\"css-1o26r9v\")\n",
    "    \n",
    "    details_list = []\n",
    "\n",
    "    for i in range(len(get_books)):\n",
    "        title = get_titles[i].get_text()\n",
    "        author = get_authors[i].get_text()[3:]\n",
    "        rank = \"#\" + str(i+1)\n",
    "        date = get_date.get_text()\n",
    "        weeks_on = get_positions[i].get_text().split(' ')[0]\n",
    "        \n",
    "        details = [title, author, rank, date, weeks_on]\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "    \n",
    "book_details = get_book_details(test_url)\n",
    "\n",
    "for x in book_details:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-agreement",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "african-worship",
   "metadata": {},
   "source": [
    "takes page, links to prev page, takes info, links to prev page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "individual-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/books/best-sellers/2021/01/10/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n",
      "/books/best-sellers/2021/01/03/combined-print-and-e-book-fiction/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_url = '/books/best-sellers/2021/01/10/combined-print-and-e-book-fiction/'\n",
    "\n",
    "def get_links(first_link, n):\n",
    "    page = requests.get('https://www.nytimes.com' + first_link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    links_list = [first_link]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        get_prev_link = soup.find(\"div\", class_=\"css-iouqpc\")\n",
    "        prev_link = get_prev_link.a.attrs['href']\n",
    "        links_list.append(prev_link) \n",
    "        \n",
    "    \n",
    "    return links_list\n",
    "\n",
    "\n",
    "test = get_links(test_url, 10)\n",
    "for x in test:\n",
    "    print(x + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "capable-railway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE FOUR WINDS', 'Kristin Hannah', '#1', 'February 28, 2021', '2']\n",
      "['FAITHLESS IN DEATH', 'J.D. Robb', '#2', 'February 28, 2021', 'New']\n",
      "['FIREFLY LANE', 'Kristin Hannah', '#3', 'February 28, 2021', '4']\n",
      "['RECKLESS ROAD', 'Christine Feehan', '#4', 'February 28, 2021', 'New']\n",
      "['THE DUKE AND I', 'Julia Quinn', '#5', 'February 28, 2021', '7']\n",
      "['THE MIDNIGHT LIBRARY', 'Matt Haig', '#6', 'February 28, 2021', '11']\n",
      "['RAFAEL', 'Laurell K. Hamilton', '#7', 'February 28, 2021', 'New']\n",
      "['THE SANATORIUM', 'Sarah Pearse', '#8', 'February 28, 2021', '2']\n",
      "['THE VISCOUNT WHO LOVED ME', 'Julia Quinn', '#9', 'February 28, 2021', '7']\n",
      "['THE VANISHING HALF', 'Brit Bennett', '#10', 'February 28, 2021', '37']\n",
      "['THE RUSSIAN', 'James Patterson and James O. Born', '#11', 'February 28, 2021', '3']\n",
      "['THE PARIS LIBRARY', 'Janet Skeslien Charles', '#12', 'February 28, 2021', 'New']\n",
      "['THE INVISIBLE LIFE OF ADDIE LARUE', 'V.E. Schwab', '#13', 'February 28, 2021', '14']\n",
      "['ROMANCING MISTER BRIDGERTON', 'Julia Quinn', '#14', 'February 28, 2021', '5']\n",
      "['WHERE THE CRAWDADS SING', 'Delia Owens', '#15', 'February 28, 2021', '121']\n"
     ]
    }
   ],
   "source": [
    "test_url = '/books/best-sellers/2021/02/28/combined-print-and-e-book-fiction/'\n",
    "\n",
    "def get_book_details(week_url):\n",
    "    \n",
    "    page = requests.get('https://www.nytimes.com/' + week_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    get_ol = soup.find(\"ol\", class_=\"css-12yzwg4\")\n",
    "\n",
    "    get_books = get_ol.find_all(\"li\", class_=\"css-13y32ub\")\n",
    "    get_titles = soup.find_all(\"h3\", class_=\"css-5pe77f\")\n",
    "    get_authors = soup.find_all(\"p\", class_=\"css-hjukut\")\n",
    "    get_date = soup.find(\"time\", class_=\"css-6068ga\")\n",
    "    get_weeks = soup.find_all(\"p\", class_=\"css-1o26r9v\")\n",
    "\n",
    "    get_prev_link = soup.find(\"div\", class_=\"css-iouqpc\")\n",
    "    prev_link = get_prev_link.a.attrs['href']\n",
    "\n",
    "    details_list = []\n",
    "\n",
    "    for i in range(len(get_books)):\n",
    "        title = get_titles[i].get_text()\n",
    "        author = get_authors[i].get_text()[3:]\n",
    "        rank = \"#\" + str(i+1)\n",
    "        date = get_date.get_text()\n",
    "        weeks_on = get_weeks[i].get_text().split(' ')[0]\n",
    "        \n",
    "        details = [title, author, rank, date, weeks_on]\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "    \n",
    "book_details = get_book_details(test_url)\n",
    "\n",
    "for x in book_details:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-match",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
